{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['KEYS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['KEYS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://c0efd5e135bc:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f868887add8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "input_data = \"s3a://udacity-dend/\"\n",
    "output_data = \"s3a://arda-udacity-song-data/\"\n",
    "spark\n",
    "#process_song_data(spark, input_data, output_data)    \n",
    "#process_log_data(spark, input_data, output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|         artist_id|artist_latitude|     artist_location|artist_longitude|         artist_name| duration|num_songs|           song_id|               title|year|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "|ARTC1LV1187B9A4858|        51.4536|Goldsmith's Colle...|        -0.01802|  The Bonzo Dog Band|301.40036|        1|SOAFBCP12A8C13CC7D|King Of Scurf (20...|1972|\n",
      "|ARA23XO1187B9AF18F|       40.57885|Carteret, New Jersey|       -74.21956|     The Smithereens|  192.522|        1|SOKTJDS12AF72A25E5|Drown In My Own T...|   0|\n",
      "|ARSVTNL1187B992A91|       51.50632|     London, England|        -0.12714|       Jonathan King|129.85424|        1|SOEKAZG12AB018837E|I'll Slap Your Fa...|2001|\n",
      "|AR73AIO1187B9AD57B|       37.77916|   San Francisco, CA|      -122.42005|   Western Addiction|118.07302|        1|SOQPWCR12A6D4FB2A3|A Poor Recipe For...|2005|\n",
      "|ARXQBR11187B98A2CC|           null|  Liverpool, England|            null|Frankie Goes To H...|821.05424|        1|SOBRKGM12A8C139EF6|Welcome to the Pl...|1985|\n",
      "+------------------+---------------+--------------------+----------------+--------------------+---------+---------+------------------+--------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#def process_song_data(spark, input_data, output_data):\n",
    "\n",
    "# get filepath to song data file\n",
    "song_data = input_data + 'song_data/A/A/A/*.json'\n",
    "\n",
    "# read song data file\n",
    "df = spark.read.json(song_data)\n",
    "df.show(5)\n",
    "\n",
    "# extract columns to create songs table\n",
    "#songs_table = df.select(['song_id', 'title', 'artist_id', 'year', 'duration']).dropDuplicates()\n",
    "\n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "#songs_table.write.parquet(os.path.join(output_data, 'songs'), partitionBy=['year', 'artist_id'])\n",
    "\n",
    "# extract columns to create artists table\n",
    "#artists_table = df.select(['artist_id', 'artist_name', 'artist_location', 'artist_latitude','artist_longitude'])\n",
    "#artists_table_new_names = artists_table.toDF('artist_id','name','location','latitude','longitude').dropDuplicates()\n",
    "\n",
    "# write artists table to parquet files\n",
    "#artists_table_new_names.write.parquet(os.path.join(output_data, 'artists')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6820"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#def process_log_data(spark, input_data, output_data):\n",
    "# get filepath to log data file\n",
    "#log_data = input_data + \"log_data/*/*/*.json\"\n",
    "\n",
    "# read log data file\n",
    "#df = spark.read.json(log_data)\n",
    "#df.show(5)\n",
    "\n",
    "# filter by actions for song plays\n",
    "#df = df.filter(df.page == 'NextSong')\n",
    "\n",
    "# extract columns for users table    \n",
    "#users_table = df.select(['userId', 'firstName', 'lastName', 'gender', 'level'])\n",
    "#users_table_new_names = users_table.toDF('user_id','first_name','last_name','gender','level').dropDuplicates()\n",
    "\n",
    "# write users table to parquet files\n",
    "#users_table.write.parquet(os.path.join(output_data, 'users'))\n",
    "\n",
    "# create timestamp column from original timestamp column\n",
    "#get_timestamp = F.udf(lambda ts: datetime.fromtimestamp(ts/1000).isoformat())\n",
    "#df = df.withColumn('start_time', get_timestamp('ts').cast(TimestampType()))\n",
    "#df.show(5)\n",
    "#df.printSchema()\n",
    "\n",
    "# extract columns to create time table\n",
    "#time_table = df.select('start_time').dropDuplicates()\n",
    "#time_table = time_table.withColumn('hour', F.hour('start_time'))\n",
    "#time_table = time_table.withColumn('day', F.dayofmonth('start_time'))\n",
    "#time_table = time_table.withColumn('week', F.weekofyear('start_time'))\n",
    "#time_table = time_table.withColumn('month', F.month('start_time'))\n",
    "#time_table = time_table.withColumn('year', F.year('start_time'))\n",
    "#time_table = time_table.withColumn('weekday', F.dayofweek('start_time'))\n",
    "\n",
    "#time_table.show(10)\n",
    "#time_table.printSchema()\n",
    "#time_table.count()\n",
    "\n",
    "# write time table to parquet files partitioned by year and month\n",
    "#time_table.write.parquet(os.path.join(output_data, 'time'), partitionBy=['year', 'month'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in song data to use for songplays table\n",
    "#song_df = spark.read.json(input_data + 'song_data/A/A/A/*.json')\n",
    "\n",
    "# take a look before join\n",
    "#song_df.printSchema()\n",
    "#df.printSchema()\n",
    "\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "#joined_df = song_df.join(df, song_df.artist_name == df.artist, 'inner')\n",
    "#joined_df = joined_df.withColumn('songplay_id', F.monotonically_increasing_id())\n",
    "#joined_df.show(5)\n",
    "#joined_df.printSchema()\n",
    "\n",
    "#songplays_table = joined_df.select(['songplay_id','start_time','userId','level','song_id','artist_id','sessionId','location','userAgent','year']).dropDuplicates()\n",
    "#songplays_table = songplays_table.withColumn('month', F.month('start_time'))\n",
    "#songplays_table = songplays_table.withColumnRenamed('userId','user_id')\n",
    "#songplays_table = songplays_table.withColumnRenamed('sessionId','session_id')\n",
    "#songplays_table = songplays_table.withColumnRenamed('userAgent','user_agent')\n",
    "#songplays_table.show(10)\n",
    "#songplays_table.printSchema()\n",
    "\n",
    "songplays_table.count()\n",
    "\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "#songplays_table.write.parquet(os.path.join(output_data, 'songplays'), partitionBy=['year', 'month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
